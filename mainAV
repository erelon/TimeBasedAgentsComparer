#mainAV.py
# AV edit for more simulation detais and output in a csv file.
# 8-jun-25
#note - file output in the same drive as the code is stored.

import csv
from datetime import datetime
import statistics



from collections import defaultdict

from env import StatelessEnv
from learners import RandomAgent, QLearningAgent, RLAgent, ContinuesMAB, OracleAgent, MAB


def train_single_agent(agent, env, episodes=200, eval_steps=20, seed=42):
    """
    Train the agent in the given environment for a specified number of episodes.
    """
    env.set_seed(seed)
    env.reset()
    state = env.get_state()  # In a stateless environment, state is not used
    for episode in range(episodes):
        action = agent.act(state)
        time, reward = env.get_reward(agent, action)
        new_state = env.get_state()
        agent.learn(state, action, reward, new_state, time)
        state = new_state

    rewards = []
    for _ in range(eval_steps):
        action = agent.eval(state)
        time, reward = env.get_reward(agent, action)
        state = env.get_state()
        rewards.append(reward)

    return sum(rewards) / len(rewards)


if __name__ == '__main__':
    env = StatelessEnv("Stateless Environment")

    oracle = OracleAgent(name="Oracle Agent", action_space=env.get_action_space(), env_secret=env.secret())
    q_agent = QLearningAgent(name="QLearning Agent", action_space=env.get_action_space())
    random_agent = RandomAgent(name="Random Agent", action_space=env.get_action_space())
    r_agent_with_trick = RLAgent(name="RL Agent with trick", action_space=env.get_action_space())
    r_agent_without_trick = RLAgent(name="RL Agent without trick", action_space=env.get_action_space(),
                                    with_rho_trick=False)
    mab = MAB(name="MAB", action_space=env.get_action_space())
    c_mab = ContinuesMAB(name="Continues MAB", action_space=env.get_action_space())

    agents = [oracle, random_agent, q_agent, r_agent_with_trick, r_agent_without_trick, c_mab, mab]
    episodes = 10000
    eval_steps = 495


   # Get the current date and time, and format it as a string
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'agent_results_{timestamp}.csv'


    with open(filename, mode='w', newline='') as file:
        writer = csv.writer(file)
        # Write the header
        writer.writerow(['Agent Name', 'State', 'Best Action Ratio', 'Average Reward', 'min rwd', 'max rwd', 
            'std dev rwd', 'Number of Steps', 'episodes', 'max-min', 'percent_Oracle'])

        for agent in agents:
            best_action_per_state = defaultdict(list)
            avg_rewards = []
            for i in range(1, 100):
                agent.reset()
                avg_reward = train_single_agent(agent, env, episodes=episodes, eval_steps=eval_steps, seed=i)
                avg_rewards.append(avg_reward)
                for state in agent.q_table:
                    actions = agent.q_table[state]
                    best_action = max(actions, key=actions.get)
                    best_action_per_state[state].append(best_action)
                
              
            avg_reward_over_steps = sum(avg_rewards) / len(avg_rewards)

            min_reward = min(avg_rewards)
            max_reward = max(avg_rewards)
            std_dev_reward = statistics.stdev(avg_rewards)
            range_reward = max_reward-min_reward

            if agent == oracle:
                
                # For the Oracle agent, the best action ratio is always 1.0
                # For the oracle state is set to 1.0:
                writer.writerow([agent.name,1.0, 1.0, avg_reward_over_steps, min_reward, max_reward, 
                std_dev_reward,eval_steps, episodes, range_reward, 1.0])
                Oracle_avg_reward_over_steps = avg_reward_over_steps

            elif agent == random_agent:
                #same for Random_agent
                writer.writerow([agent.name,1.0, 1.0, avg_reward_over_steps, min_reward, max_reward, 
                std_dev_reward,eval_steps, episodes, range_reward, avg_reward_over_steps/Oracle_avg_reward_over_steps])
            
            else:
                for state, actions in best_action_per_state.items():
                    best_action_ratio = len([i for i in actions if i == oracle.act(state)]) / len(actions)
                    writer.writerow([agent.name, state, best_action_ratio, avg_reward_over_steps,min_reward, max_reward, 
                    std_dev_reward, eval_steps, episodes, range_reward, avg_reward_over_steps/Oracle_avg_reward_over_steps])     
